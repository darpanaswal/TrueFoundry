{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGILw4o6cNfZ",
    "outputId": "8a34441a-8a72-4771-829b-60bfce88c0ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import uvicorn\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC    \n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.corpus import stopwords   \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier       \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UnOVHo60c61k"
   },
   "outputs": [],
   "source": [
    "class model:\n",
    "    def read_dataset(data):\n",
    "        dataset = pd.read_csv(data)\n",
    "        dataset.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "        dataset.columns = ['sentiment', 'review']\n",
    "        dataset.review = dataset.review.str.split(\" \",1).str[1]\n",
    "        encoded_dict = {'negative': 0, 'positive': 1}\n",
    "        dataset['sentiment'] = dataset.sentiment.map(encoded_dict)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def clean_dataset(column):\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.remove('not')\n",
    "        stop_words.remove('or')\n",
    "        for row in column:\n",
    "        # Split CamelCase words\n",
    "            row = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', str(row))).split()\n",
    "        # Remove special characters and numbers \n",
    "            row = re.sub('[^a-zA-Z]', ' ', str(row))\n",
    "        # Remove Repeated words\n",
    "            row = re.sub(r\"\\\\b(\\\\w+)(?:\\\\W+\\\\1\\\\b)+\", \"\", str(row))\n",
    "        # Replace tabs and newlines with a single space\n",
    "            row = re.sub(\"(\\\\t)\", \" \", str(row))\n",
    "            row = re.sub(\"(\\\\r)\", \" \", str(row))\n",
    "            row = re.sub(\"(\\\\n)\", \" \", str(row))\n",
    "        # Remove single alphabets\n",
    "            row = re.sub(r'(?:^| )\\w(?:$| )', ' ', str(row)).strip()\n",
    "            row = row.split()\n",
    "            row = [word for word in row if not word in set(stop_words)]\n",
    "            row = ' '.join(row)\n",
    "            row = row.lower()\n",
    "            yield row\n",
    "\n",
    "    \n",
    "    def reviews(data):\n",
    "        dataset = model.read_dataset(data)\n",
    "        reviews = model.clean_dataset(dataset.review)\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
    "        reviews = [str(doc) for doc in nlp.pipe(reviews, batch_size = 128)]\n",
    "        cv = CountVectorizer(max_features = 1500)\n",
    "        X = cv.fit_transform(reviews).toarray()\n",
    "        y = dataset.sentiment\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "        return cv, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    def logit(X_train, y_train):\n",
    "        parameters = [{'penalty':['l2','none'], 'solver':['newton-cg','lbfgs','saga'], 'C':[10,1.0,0.1,0.01]}]\n",
    "\n",
    "        classifier = GridSearchCV(estimator = LogisticRegression(random_state = 0),\n",
    "                           param_grid = parameters, scoring = 'accuracy',\n",
    "                           cv = 10, n_jobs = -1)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        best_accuracy = classifier.best_score_\n",
    "        best_parameters = classifier.best_params_\n",
    "        return classifier, best_accuracy, best_parameters\n",
    "\n",
    "\n",
    "    def tree(X_train, y_train):\n",
    "        parameters = [{'criterion':['gini','entropy'], 'splitter':['best','random'], 'max_features':['sqrt','log2']}]\n",
    "\n",
    "        classifier = GridSearchCV(estimator = DecisionTreeClassifier(random_state = 0),\n",
    "                           param_grid = parameters, scoring = 'accuracy',\n",
    "                           cv = 10, n_jobs = -1)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        best_accuracy = classifier.best_score_\n",
    "        best_parameters = classifier.best_params_\n",
    "        return classifier, best_accuracy, best_parameters\n",
    "\n",
    "\n",
    "    def forest(X_train, y_train):\n",
    "        parameters = [{'n_estimators':[20,40,60,80], 'criterion':['gini','entropy'], 'max_features':['auto','log2']}]\n",
    "\n",
    "        classifier = GridSearchCV(estimator = RandomForestClassifier(random_state = 0),\n",
    "                           param_grid = parameters, scoring = 'accuracy',\n",
    "                           cv = 10, n_jobs = -1)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        best_accuracy = classifier.best_score_\n",
    "        best_parameters = classifier.best_params_\n",
    "        return classifier, best_accuracy, best_parameters\n",
    "\n",
    "\n",
    "    def SV(X_train, y_train):\n",
    "        parameters = [{'kernel': ['linear','poly','sigmoid'], 'gamma': [0.001, 0.0001], 'C':[10,1.0,0.1,0.01]}]\n",
    "\n",
    "        classifier = GridSearchCV(estimator = SVC(random_state = 0),\n",
    "                           param_grid = parameters, scoring = 'accuracy',\n",
    "                           cv = 10, n_jobs = -1)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        best_accuracy = classifier.best_score_\n",
    "        best_parameters = classifier.best_params_\n",
    "        return classifier, best_accuracy, best_parameters\n",
    "\n",
    "\n",
    "    def XGB(X_train, y_train):\n",
    "        parameters = [{'max_depth':[4,6,8,10], 'n_estimators':[20,40,60,80]}]\n",
    "\n",
    "        classifier = GridSearchCV(estimator = XGBClassifier(random_state = 0),\n",
    "                           param_grid = parameters, scoring = 'accuracy',\n",
    "                           cv = 10, n_jobs = -1)\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        best_accuracy = classifier.best_score_\n",
    "        best_parameters = classifier.best_params_\n",
    "        return classifier, best_accuracy, best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "DE0hyhIsdAIy",
    "outputId": "30deae43-dbbe-4983-99c1-df859a9f6b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9146453311946269 {'C': 1.0, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.8381721564820156 {'criterion': 'entropy', 'max_features': 'sqrt', 'splitter': 'best'}\n",
      "0.9045706640777063 {'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 60}\n",
      "0.904029304029304 {'max_depth': 10, 'n_estimators': 80}\n",
      "0.9064136567657695 {'C': 1.0, 'gamma': 0.001, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "data = 'airline_sentiment_analysis.csv'\n",
    "cv, X_train, X_test, y_train, y_test = model.reviews(data)\n",
    "logit_classifier, logit_acc, logit_param = model.logit(X_train, y_train)\n",
    "print(logit_acc, logit_param)\n",
    "tree_classifier, tree_acc, tree_param = model.tree(X_train, y_train)\n",
    "print(tree_acc, tree_param)\n",
    "forest_classifier, forest_acc, forest_param = model.forest(X_train, y_train)\n",
    "print(forest_acc, forest_param)\n",
    "xgb_classifier, xgb_acc, xgb_param = model.XGB(X_train, y_train)\n",
    "print(xgb_acc, xgb_param)\n",
    "svc_classifier, svc_acc, svc_param = model.SV(X_train, y_train)\n",
    "print(svc_acc, svc_param)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f163997e9f8e499c00e0e6b9e906414411e2127b46913a7e019f286d4ee483"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
